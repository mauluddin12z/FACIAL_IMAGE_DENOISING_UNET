{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4RoNdk45e8-x"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "sys.path.insert(0, 'D:\\Machine Learning Project\\FACIAL_IMAGE_DENOISING_UNET\\library')\n",
    "from utils import (\n",
    "    preprocess_and_load_data,\n",
    "    add_noise,\n",
    "    display_image,\n",
    "    ssim,\n",
    "    psnr,\n",
    "    create_experiment_notes,\n",
    "    tensorboard_callback,\n",
    "    early_stopping_callback,\n",
    "    checkpoint_callback,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ogc2TNHPe8-y"
   },
   "source": [
    "## **Data Collecting**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0hc_sPn7MTTW"
   },
   "outputs": [],
   "source": [
    "with tf.device(\"/device:CPU:0\"):\n",
    "    dataset_path = \"/MACHINE LEARNING/DATASET/faceImage\"\n",
    "    file_names = os.listdir(dataset_path)\n",
    "\n",
    "    total_samples = len(file_names)\n",
    "    num_train_samples = 8000\n",
    "    num_test_samples = 2000\n",
    "\n",
    "    train_file_names = file_names[:num_train_samples]\n",
    "    test_file_names = file_names[\n",
    "        num_train_samples : num_train_samples + num_test_samples\n",
    "    ]\n",
    "\n",
    "    print(f\"Number of samples: {total_samples}\")\n",
    "    print(f\"Number of training samples: {num_train_samples}\")\n",
    "    print(f\"Number of testing samples: {num_test_samples}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RbkHzkWZfUI1"
   },
   "source": [
    "## **Data Preparation**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y66Y8wakgDei"
   },
   "source": [
    "### **Load and preprocess training and testing data**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AJ1irxi83yIP"
   },
   "outputs": [],
   "source": [
    "with tf.device(\"/device:CPU:0\"):\n",
    "    Y_train = preprocess_and_load_data(dataset_path, train_file_names)\n",
    "    Y_test = preprocess_and_load_data(dataset_path, test_file_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L09kRyEsgF3h"
   },
   "source": [
    "### **Add noise to training and testing data**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device(\"/device:CPU:0\"):\n",
    "    noise_factor_range_set = [0.3, 0.3, 0.01]\n",
    "    X_train = add_noise(Y_train, noise_factor_range_set)\n",
    "    X_test = add_noise(Y_test, noise_factor_range_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GBucs5uGgLok"
   },
   "source": [
    "### **Display some examples of original data and Noisy data**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jPagP3I-34Be"
   },
   "outputs": [],
   "source": [
    "labels = [\"Original Image\", \"Noisy Image\"]\n",
    "with tf.device(\"/device:CPU:0\"):\n",
    "    rng = np.random.default_rng()\n",
    "    random_indices = rng.choice(num_train_samples, size=10)\n",
    "    display_image(Y_train[random_indices], X_train[random_indices], labels=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0roGQxHGgQNa"
   },
   "source": [
    "## **Modelling**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Convolutional Autoencoder Model Architecture**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Create the autoencoder model**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_architecture.UNet_Architecture import autoencoder\n",
    "\n",
    "with tf.device(\"/device:CPU:0\"):\n",
    "    model_name = \"U-Net\"\n",
    "    learning_rate = 1e-4\n",
    "    optimizer = tf.optimizers.Adam(learning_rate=learning_rate)\n",
    "    model = autoencoder(optimizer)\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(\n",
    "    model, to_file=f\"architecture_img/{model_name}.png\", show_shapes=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p_jHA9f0gZnJ"
   },
   "source": [
    "### **Model Callbacks**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device(\"/device:CPU:0\"):\n",
    "    checkpoint = checkpoint_callback(f\"models/{model_name}.h5\")\n",
    "    early_stopping = early_stopping_callback()\n",
    "    log_dir, experiment_dir, tensorboard = tensorboard_callback(\"Image Denoising Log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Model Training**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zC7V6Y_fg8oB"
   },
   "source": [
    "### **Train the model with noisy training data**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vmUJh0N7UVhB",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "epochs = 100\n",
    "batch_size = 8\n",
    "validation_split = 0.2\n",
    "\n",
    "start_time = time.time()\n",
    "history = model.fit(\n",
    "    x=X_train,\n",
    "    y=Y_train,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    validation_split=validation_split,\n",
    "    callbacks=[checkpoint, early_stopping, tensorboard],\n",
    ")\n",
    "end_time = time.time()\n",
    "\n",
    "execution_time = end_time - start_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iARj2r41hHMU"
   },
   "source": [
    "### **Plot the Loss, SSIM, and PSNR Graph**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EN51_xzXUHri"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot Training Loss and Validation Loss\n",
    "plt.figure(figsize=(18, 6))\n",
    "plt.subplot(131)\n",
    "plt.plot(history.history[\"loss\"], label=\"Training Loss\")\n",
    "plt.plot(history.history[\"val_loss\"], label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Training Loss and Validation Loss\")\n",
    "\n",
    "# Plot Training SSIM and Validation SSIM\n",
    "plt.subplot(132)\n",
    "plt.plot(history.history[\"ssim\"], label=\"SSIM\")\n",
    "plt.plot(history.history[\"val_ssim\"], label=\"Validation SSIM\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"SSIM\")\n",
    "plt.legend()\n",
    "plt.title(\"Training SSIM and Validation SSIM\")\n",
    "\n",
    "# Plot Training PSNR and Validation PSNR\n",
    "plt.subplot(133)\n",
    "plt.plot(history.history[\"psnr\"], label=\"PSNR\")\n",
    "plt.plot(history.history[\"val_psnr\"], label=\"Validation PSNR\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"PSNR\")\n",
    "plt.legend()\n",
    "plt.title(\"Training PSNR and Validation PSNR\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Model Evaluation**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved model with custom metric functions\n",
    "with tf.device(\"/device:CPU:0\"):\n",
    "    best_model = tf.keras.models.load_model(\n",
    "        f\"models/{model_name}.h5\",\n",
    "        custom_objects={\"ssim\": ssim, \"psnr\": psnr},\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device(\"/device:CPU:0\"):\n",
    "    # Evaluate the model on the test data\n",
    "    evaluation = best_model.evaluate(X_test, Y_test, batch_size=16)\n",
    "\n",
    "    # Extract the metrics from the results\n",
    "    loss, ssim_value, psnr_value = evaluation[0], evaluation[1], evaluation[2]\n",
    "\n",
    "    # Display the results\n",
    "    print(f\"Test Loss (MSE): {loss}\")\n",
    "    print(f\"Test SSIM: {ssim_value}\")\n",
    "    print(f\"Test PSNR: {psnr_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Experiment Logs**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device(\"/device:CPU:0\"):\n",
    "    model_summary_text = []\n",
    "    for layer in model.layers:\n",
    "        layer_summary = (\n",
    "            f\"{layer.name} ({layer.__class__.__name__}) {layer.output_shape}\"\n",
    "        )\n",
    "        model_summary_text.append(layer_summary)\n",
    "\n",
    "    model_summary_text = \"<br/>\".join(model_summary_text)\n",
    "\n",
    "    experiment_notes = create_experiment_notes(\n",
    "        model_name=model_name,\n",
    "        model=model_summary_text,\n",
    "        total_train_data=num_train_samples,\n",
    "        total_test_data=num_test_samples,\n",
    "        noise_factor_range_set=noise_factor_range_set,\n",
    "        learning_rate=learning_rate,\n",
    "        optimizer=optimizer.__class__.__name__,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_split=validation_split,\n",
    "        loss=loss,\n",
    "        ssim_value=ssim_value,\n",
    "        psnr_value=psnr_value,\n",
    "        execution_time=execution_time,\n",
    "    )\n",
    "\n",
    "    text_file_writer = tf.summary.create_file_writer(log_dir + \"/text/\")\n",
    "    with text_file_writer.as_default():\n",
    "        tf.summary.text(experiment_dir, experiment_notes, step=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UOECXqmshTyy"
   },
   "source": [
    "### **Display test, noisy test data, and predictions image**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3eCIfE09Uu3Q"
   },
   "outputs": [],
   "source": [
    "with tf.device(\"/device:CPU:0\"):\n",
    "    rng = np.random.default_rng()\n",
    "    random_indices = rng.choice(num_test_samples, size=10)\n",
    "    labels = [\"Original Image\", \"Noisy Image\", \"Predictions\"]\n",
    "    predictions = best_model.predict(X_test[random_indices])\n",
    "    display_image(\n",
    "        Y_test[random_indices], X_test[random_indices], predictions, labels=labels\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
